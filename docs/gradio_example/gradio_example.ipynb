{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gradio Example\n",
    "\n",
    "Doing this to understand how it works.\n",
    "\n",
    "✍️ Tip: When developing locally, you can run your Gradio app in hot reload mode, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in gradio before the name of the file instead of python. In the example above, you would type: gradio app.py in your terminal.\n",
    "\n",
    "Note: if gradio isn't working properly check if libraries are imported correctly!!!\n",
    "\n",
    "https://www.gradio.app/guides/creating-a-chatbot-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gradio as gr\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read openai key\n",
    "with open(r\"C:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\GPT_api_key.txt\") as f:\n",
    "    openai_api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(query: str) -> str:\n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_gpt(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Interface class has three core arguments:\n",
    "\n",
    "- fn: the function to wrap a user interface (UI) around\n",
    "- inputs: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n",
    "- outputs: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Interface class\n",
    "demo = gr.Interface(\n",
    "    fn=ask_gpt,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"textbox\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True)  # Share your demo with just 1 extra parameter share\n",
    "# a public URL will be generated for your demo in a matter of seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with gr.ChatInterface(), the first thing you should do is define your chat function. Your chat function should take two arguments: message and then history (the arguments can be named anything, but must be in this order).\n",
    "\n",
    "- message: a str representing the user’s input.\n",
    "- history: a list of list representing the conversations up until that point. Each inner list consists of two str representing a pair: [user input, bot response].\n",
    "\n",
    "Your function should return a single string response, which is the bot’s response to the particular user input message. Your function can take into account the history of messages, as well as the current message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def random_response(message, history):\n",
    "    return random.choice([\"Yes\", \"No\"])\n",
    "\n",
    "gr.ChatInterface(random_response).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_gpt(message: str, history) -> str:\n",
    "    \n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(ask_gpt).launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMPLE OF STREAMING\n",
    "\n",
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "def slow_echo(message, history):\n",
    "    for i in range(len(message)):\n",
    "        time.sleep(0.3)\n",
    "        yield \"You typed: \" + message[: i+1]\n",
    "\n",
    "gr.ChatInterface(slow_echo).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1591, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1188, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 502, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 628, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 487, in _stream_fn\n",
      "    first_response = await async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 502, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 495, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 478, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Michelle\\AppData\\Local\\Temp\\ipykernel_23472\\612024847.py\", line 20, in ask_gpt\n",
      "    time.sleep(0.01)\n",
      "    ^^^^\n",
      "NameError: name 'time' is not defined\n"
     ]
    }
   ],
   "source": [
    "# streaming enabled by letter\n",
    "\n",
    "def ask_gpt(message: str, history) -> str:\n",
    "    \n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    for i in range(len(response)):\n",
    "        time.sleep(0.01)\n",
    "        yield response[:i+1]\n",
    "\n",
    "gr.ChatInterface(ask_gpt).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADD CHAT HISTORY \n",
    "system_prompt = {\"role\":\"system\", \"content\":\"You speak with Australian slang.\"}\n",
    "    \n",
    "def ask_gpt(message: str, history) -> str:\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[system_prompt] + history_openai_format,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    #return response\n",
    "    for i in range(len(response)):\n",
    "        time.sleep(0.01)\n",
    "        yield response[:i+1]\n",
    "\n",
    "gr.ChatInterface(ask_gpt).launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching examples at: 'C:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\docs\\gradio_example\\gradio_cached_examples\\31'\n",
      "Caching example 1/3\n",
      "Caching example 2/3\n",
      "Caching example 3/3\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "# Read openai key\n",
    "with open(r\"C:\\Users\\Michelle\\Data_Science_Code\\RAG-chatbot\\src\\GPT_api_key.txt\") as f:\n",
    "    openai_api_key = f.read()\n",
    "\n",
    "# FINAL TRAIL: ADDED CHAT HISTORY + STREAMING FOR OPENAI API + FEATURES \n",
    "system_prompt = {\"role\":\"system\", \"content\":\"You speak with Australian slang.\"}\n",
    "    \n",
    "def ask_gpt(message: str, history) -> str:\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[system_prompt] + history_openai_format,\n",
    "        max_tokens=150,\n",
    "        stream=True,\n",
    "    )\n",
    "    partial_message = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            partial_message = partial_message + chunk.choices[0].delta.content\n",
    "            yield partial_message\n",
    "            \n",
    "gr.ChatInterface(\n",
    "    ask_gpt, \n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a question\", container=False, scale=7),\n",
    "    title=\"Aussie Bot\",\n",
    "    description=\"Ask our Aussie Bot any question!\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"What is the capital of Greece?\", \"Is the ocean blue?\", \"Cats or dogs?\"],\n",
    "    cache_examples=True,\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
